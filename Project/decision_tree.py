# -*- coding: utf-8 -*-
"""decision_tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IpOl5tUYzuGr3I5ziruMqe7_hQGRHX-f
"""

def preprocess_data(df):
    df = df.replace('?', np.nan)

    for column in df.columns:
        if df[column].isna().sum() > 0:
            most_common = df[column].mode()[0]
            df[column] = df[column].fillna(most_common)

    categorical_columns = df.select_dtypes(include=['object']).columns
    for column in categorical_columns:
        df[column] = pd.Categorical(df[column]).codes

    return df

def create_node(value=None):
    return {'value': value, 'left': None, 'right': None, 'feature': None, 'threshold': None}

def is_leaf_node(node):
    return node['value'] is not None

def determine_feature_types(X):
    feature_types = []
    for i in range(X.shape[1]):
        unique_values = np.unique(X[:, i])
        if isinstance(unique_values[0], (int, float)) and len(unique_values) > 10:
            feature_types.append("numerical")
        else:
            feature_types.append("categorical")
    return feature_types

def split_data(X, feature, threshold):
    left_indices = np.where(X[:, feature] <= threshold)[0]
    right_indices = np.where(X[:, feature] > threshold)[0]
    return left_indices, right_indices

def calculate_entropy(y):
    label_counts = Counter(y)
    total = len(y)
    entropy_value = 0.0
    for count in label_counts.values():
        p = count / total
        if p > 0:
            entropy_value -= p * np.log2(p)
    return entropy_value

def information_gain(y, left_y, right_y):
    n = len(y)
    parent_entropy = calculate_entropy(y)
    left_entropy = calculate_entropy(left_y)
    right_entropy = calculate_entropy(right_y)
    child_entropy = (len(left_y) / n) * left_entropy + (len(right_y) / n) * right_entropy
    return parent_entropy - child_entropy

def best_split(X, y, feature_types, num_thresholds=10):
    best_gain = -np.inf
    best_feature = None
    best_threshold = None
    n_features = X.shape[1]

    for feature in range(n_features):
        if feature_types[feature] == "numerical":
            unique_values = np.unique(X[:, feature])
            if len(unique_values) > num_thresholds:
                thresholds = np.quantile(unique_values, np.linspace(0, 1, num_thresholds + 2)[1:-1])
            else:
                thresholds = unique_values

            for threshold in thresholds:
                left_indices, right_indices = split_data(X, feature, threshold)
                if len(left_indices) == 0 or len(right_indices) == 0:
                    continue
                gain = information_gain(y, y[left_indices], y[right_indices])
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold
        else:
            unique_values = np.unique(X[:, feature])
            for value in unique_values:
                left_indices, right_indices = split_data(X, feature, value)
                if len(left_indices) == 0 or len(right_indices) == 0:
                    continue
                gain = information_gain(y, y[left_indices], y[right_indices])
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = value

    return best_feature, best_threshold

def build_tree(X, y, feature_types, max_depth=10, depth=0):
    if len(np.unique(y)) == 1 or depth == max_depth:
        return create_node(value=np.mean(y))

    best_feature, best_threshold = best_split(X, y, feature_types)

    if best_feature is None:
        return create_node(value=np.mean(y))

    left_indices, right_indices = split_data(X, best_feature, best_threshold)

    if len(left_indices) == 0 or len(right_indices) == 0:
        return create_node(value=np.mean(y))

    left_node = build_tree(X[left_indices], y[left_indices], feature_types, max_depth, depth + 1)
    right_node = build_tree(X[right_indices], y[right_indices], feature_types, max_depth, depth + 1)

    parent_node = create_node()
    parent_node['feature'] = best_feature
    parent_node['threshold'] = best_threshold
    parent_node['left'] = left_node
    parent_node['right'] = right_node

    return parent_node

def predict(instance, tree):
    node = tree
    while not is_leaf_node(node):
        if instance[node['feature']] <= node['threshold']:
            node = node['left']
        else:
            node = node['right']
    return node['value']

def predict_proba(X, tree):
    return np.array([predict(instance, tree) for instance in X])

def main():
    print("Loading the data...")
    train_data = pd.read_csv('/ML2024F/train_final.csv')
    test_data = pd.read_csv('/ML2024F/test_final.csv')

    print("Preprocessing the data...")
    train_data = preprocess_data(train_data)
    test_data = preprocess_data(test_data)

    if 'income>50K' not in train_data.columns:
        raise KeyError("'income>50K' column not found in training data")

    X_train = train_data.drop(columns='income>50K').values
    y_train = train_data['income>50K'].values

    if 'income>50K' in test_data.columns:
        X_test = test_data.drop(columns='income>50K').values
    else:
        X_test = test_data.drop(columns='ID', errors='ignore').values

    test_ids = test_data.get('ID', pd.Series(dtype='int'))

    print("Determining feature types...")
    feature_types = determine_feature_types(X_train)

    print("Building the decision tree...")
    tree = build_tree(X_train, y_train, feature_types, max_depth=10)

    print("Predicting probabilities for the test set...")
    predictions = predict_proba(X_test, tree)

    print("Preparing submission file...")
    submission = pd.DataFrame({'ID': test_ids, 'Prediction': predictions})
    submission.to_csv('submission.csv', index=False)
    print("Submission file 'submission.csv' created successfully.")

if __name__ == "__main__":
    main()



#decision tree
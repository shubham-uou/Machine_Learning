# -*- coding: utf-8 -*-
"""linear_regression_batch_gradient .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FRyzHxN9pI88vbFdFbJMGEfCI101Sz1G
"""

import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

def preprocess_data(df):
    df = df.replace('?', np.nan)
    for column in df.columns:
        if df[column].dtype == 'object':
            df[column] = df[column].fillna(df[column].mode()[0])
        else:
            df[column] = df[column].fillna(df[column].mean())

    categorical_columns = df.select_dtypes(include=['object']).columns
    for column in categorical_columns:
        le = LabelEncoder()
        df[column] = le.fit_transform(df[column].astype(str))

    return df

class LinearRegressionBGD:
    def __init__(self, learning_rate=0.001, max_iter=1000):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.weights = None
        self.bias = 0

    def sigmoid(self, z):
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    def compute_loss(self, y_true, y_pred):
        return -np.mean(y_true * np.log(y_pred + 1e-15) + (1 - y_true) * np.log(1 - y_pred + 1e-15))

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for i in range(self.max_iter):
            linear_model = np.dot(X, self.weights) + self.bias
            y_pred = self.sigmoid(linear_model)

            dw = np.dot(X.T, (y_pred - y)) / n_samples
            db = np.sum(y_pred - y) / n_samples

            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

            if i % 100 == 0:
                loss = self.compute_loss(y, y_pred)
                print(f"Iteration {i}: Loss = {loss:.4f}")

    def predict_proba(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        return self.sigmoid(linear_model)

    def predict(self, X):
        probabilities = self.predict_proba(X)
        return (probabilities >= 0.5).astype(int)

def main():
    train_data = pd.read_csv('/ML2024F/train_final.csv')
    test_data = pd.read_csv('/ML2024F/test_final.csv')

    print("Train data columns:", train_data.columns)
    print("Test data columns:", test_data.columns)

    train_data = preprocess_data(train_data)
    test_data = preprocess_data(test_data)

    target_column = 'income>50K'
    X_train = train_data.drop(target_column, axis=1).values
    y_train = train_data[target_column].values

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)

    test_ids = test_data['ID']
    X_test = test_data.drop(['ID'], axis=1).values
    X_test = scaler.transform(X_test)

    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    model = LinearRegressionBGD(learning_rate=0.001, max_iter=1000)
    model.fit(X_train, y_train)

    y_pred_proba = model.predict_proba(X_val)
    auc_score = roc_auc_score(y_val, y_pred_proba)
    print(f"AUC-ROC Score on Validation Set: {auc_score:.4f}")

    y_test_pred_proba = model.predict_proba(X_test)

    output_df = pd.DataFrame({'ID': test_ids, 'Probability': y_test_pred_proba})
    output_df.to_csv('submission.csv', index=False)
    print("Predictions saved to 'submission.csv'.")

if __name__ == "__main__":
    main()

# batch gradient linear regression
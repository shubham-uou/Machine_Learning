# -*- coding: utf-8 -*-
"""pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NNDkdnCYP99OkW7ruinOQIPy94jPD7iK
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def load_data(train_path, test_path):
    train_data = pd.read_csv(train_path, header=None)
    test_data = pd.read_csv(test_path, header=None)

    X_train, y_train = train_data.iloc[:, :-1].values, train_data.iloc[:, -1].values
    X_test, y_test = test_data.iloc[:, :-1].values, test_data.iloc[:, -1].values

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    return torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32), \
           torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)

class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, activation_function, initialization):
        super(NeuralNetwork, self).__init__()
        self.layers = nn.ModuleList()
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layer = nn.Linear(prev_size, hidden_size)
            if initialization == "xavier":
                nn.init.xavier_uniform_(layer.weight)
            elif initialization == "he":
                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')
            nn.init.zeros_(layer.bias)
            self.layers.append(layer)
            prev_size = hidden_size

        self.output = nn.Linear(prev_size, output_size)
        nn.init.xavier_uniform_(self.output.weight)
        nn.init.zeros_(self.output.bias)

        if activation_function == "tanh":
            self.activation = nn.Tanh()
        elif activation_function == "relu":
            self.activation = nn.ReLU()

    def forward(self, x):
        for layer in self.layers:
            x = self.activation(layer(x))
        x = torch.sigmoid(self.output(x))
        return x

def train_and_evaluate(X_train, y_train, X_test, y_test, hidden_sizes, activation_function, initialization, depth, width, epochs=50):
    input_size = X_train.shape[1]
    output_size = 1

    model = NeuralNetwork(input_size, hidden_sizes, output_size, activation_function, initialization)
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train).squeeze()
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            test_outputs = model(X_test).squeeze()
            test_loss = criterion(test_outputs, y_test)

        print(f"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}")

    with torch.no_grad():
        predictions = (test_outputs > 0.5).float()
        accuracy = (predictions == y_test).float().mean().item()
        print(f"Depth: {depth}, Width: {width}, Activation: {activation_function}, Initialization: {initialization}, Test Accuracy: {accuracy * 100:.2f}%\n")

if __name__ == "__main__":
    X_train, y_train, X_test, y_test = load_data("/datasets/bank-note/train.csv", "/datasets/bank-note/test.csv")

    depths = [3, 5, 9]
    widths = [5, 10, 25, 50, 100]
    activations = ["tanh", "relu"]
    initializations = ["xavier", "he"]

    for activation in activations:
        for initialization in initializations:
            for depth in depths:
                for width in widths:
                    hidden_sizes = [width] * depth
                    print(f"Training with Depth={depth}, Width={width}, Activation={activation}, Initialization={initialization}")
                    train_and_evaluate(X_train, y_train, X_test, y_test, hidden_sizes, activation, initialization, depth, width)
# -*- coding: utf-8 -*-
"""back_propagation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wFvX63rP5Pg3nGVrz-Ln0DG6cj_ygBjB
"""

import numpy as np
import pandas as pd

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

def binary_cross_entropy(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))

def load_data(train_path, test_path):
    train_data = pd.read_csv(train_path, header=None)
    test_data = pd.read_csv(test_path, header=None)
    X_train, y_train = train_data.iloc[:, :-1].values, train_data.iloc[:, -1].values
    X_test, y_test = test_data.iloc[:, :-1].values, test_data.iloc[:, -1].values
    return X_train, y_train, X_test, y_test

def initialize_weights(input_size, hidden1_size, hidden2_size, output_size):
    np.random.seed(42)
    weights = {
        "W1": np.random.randn(input_size, hidden1_size) * 0.1,
        "b1": np.zeros((1, hidden1_size)),
        "W2": np.random.randn(hidden1_size, hidden2_size) * 0.1,
        "b2": np.zeros((1, hidden2_size)),
        "W3": np.random.randn(hidden2_size, output_size) * 0.1,
        "b3": np.zeros((1, output_size)),
    }
    return weights

def forward_pass(X, weights):
    Z1 = np.dot(X, weights["W1"]) + weights["b1"]
    A1 = sigmoid(Z1)

    Z2 = np.dot(A1, weights["W2"]) + weights["b2"]
    A2 = sigmoid(Z2)

    Z3 = np.dot(A2, weights["W3"]) + weights["b3"]
    A3 = sigmoid(Z3)

    cache = {"X": X, "Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2, "Z3": Z3, "A3": A3}
    return A3, cache

def backward_pass(y, weights, cache):
    m = y.shape[0]

    A3, A2, A1, X = cache["A3"], cache["A2"], cache["A1"], cache["X"]
    Z2, Z1 = cache["Z2"], cache["Z1"]

    dZ3 = A3 - y.reshape(-1, 1)
    dW3 = np.dot(A2.T, dZ3) / m
    db3 = np.sum(dZ3, axis=0, keepdims=True) / m

    dZ2 = np.dot(dZ3, weights["W3"].T) * sigmoid_derivative(Z2)
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    dZ1 = np.dot(dZ2, weights["W2"].T) * sigmoid_derivative(Z1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    gradients = {"dW3": dW3, "db3": db3, "dW2": dW2, "db2": db2, "dW1": dW1, "db1": db1}
    return gradients

def update_weights(weights, gradients, learning_rate):
    for key in weights.keys():
        weights[key] -= learning_rate * gradients["d" + key]
    return weights

def train_neural_network(X_train, y_train, X_test, y_test, hidden1_size=10, hidden2_size=8, epochs=1000, learning_rate=0.01):
    input_size = X_train.shape[1]
    output_size = 1
    weights = initialize_weights(input_size, hidden1_size, hidden2_size, output_size)

    for epoch in range(epochs):
        A3, cache = forward_pass(X_train, weights)
        loss = binary_cross_entropy(y_train, A3)

        gradients = backward_pass(y_train, weights, cache)

        weights = update_weights(weights, gradients, learning_rate)

        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")

    y_pred, _ = forward_pass(X_test, weights)
    y_pred_binary = (y_pred > 0.5).astype(int)
    accuracy = np.mean(y_pred_binary == y_test.reshape(-1, 1))
    print(f"Test Accuracy: {accuracy * 100:.2f}%")

if __name__ == "__main__":
    X_train, y_train, X_test, y_test = load_data("datasets/bank-note/train.csv", "datasets/bank-note/test.csv")

    train_neural_network(X_train, y_train, X_test, y_test, hidden1_size=10, hidden2_size=8, epochs=1000, learning_rate=0.01)
# -*- coding: utf-8 -*-
"""stochastic_gradient_descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I6RcA59Qf1HNOuG92UGeBOFJI14VNIdl
"""

import numpy as np
import pandas as pd

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

def binary_cross_entropy(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))

def load_data(train_path, test_path):
    train_data = pd.read_csv(train_path, header=None)
    test_data = pd.read_csv(test_path, header=None)
    X_train, y_train = train_data.iloc[:, :-1].values, train_data.iloc[:, -1].values
    X_test, y_test = test_data.iloc[:, :-1].values, test_data.iloc[:, -1].values

    mean = np.mean(X_train, axis=0)
    std = np.std(X_train, axis=0)
    X_train = (X_train - mean) / std
    X_test = (X_test - mean) / std

    return X_train, y_train, X_test, y_test

def initialize_weights(input_size, hidden_size, output_size):
    np.random.seed(42)
    weights = {
        "W1": np.random.randn(input_size, hidden_size) * 0.1,
        "b1": np.zeros((1, hidden_size)),
        "W2": np.random.randn(hidden_size, hidden_size) * 0.1,
        "b2": np.zeros((1, hidden_size)),
        "W3": np.random.randn(hidden_size, output_size) * 0.1,
        "b3": np.zeros((1, output_size)),
    }
    return weights

def forward_pass(X, weights):
    Z1 = np.dot(X, weights["W1"]) + weights["b1"]
    A1 = sigmoid(Z1)
    Z2 = np.dot(A1, weights["W2"]) + weights["b2"]
    A2 = sigmoid(Z2)
    Z3 = np.dot(A2, weights["W3"]) + weights["b3"]
    A3 = sigmoid(Z3)
    cache = {"X": X, "Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2, "Z3": Z3, "A3": A3}
    return A3, cache

def backward_pass(y, weights, cache):
    A3, A2, A1, X = cache["A3"], cache["A2"], cache["A1"], cache["X"]
    Z2, Z1 = cache["Z2"], cache["Z1"]

    dZ3 = A3 - y.reshape(-1, 1)
    dW3 = np.dot(A2.T, dZ3)
    db3 = np.sum(dZ3, axis=0, keepdims=True)

    dZ2 = np.dot(dZ3, weights["W3"].T) * sigmoid_derivative(Z2)
    dW2 = np.dot(A1.T, dZ2)
    db2 = np.sum(dZ2, axis=0, keepdims=True)

    dZ1 = np.dot(dZ2, weights["W2"].T) * sigmoid_derivative(Z1)
    dW1 = np.dot(X.T, dZ1)
    db1 = np.sum(dZ1, axis=0, keepdims=True)

    gradients = {"dW3": dW3, "db3": db3, "dW2": dW2, "db2": db2, "dW1": dW1, "db1": db1}
    return gradients

def update_weights(weights, gradients, learning_rate):
    for key in weights.keys():
        weights[key] -= learning_rate * gradients["d" + key]
    return weights

def learning_rate_schedule(t, gamma_0, d):
    return gamma_0 / (1 + gamma_0 / d * t)

def train_neural_network(X_train, y_train, X_test, y_test, hidden_size, gamma_0, d, epochs=50):
    input_size = X_train.shape[1]
    output_size = 1
    weights = initialize_weights(input_size, hidden_size, output_size)
    n_samples = X_train.shape[0]

    for epoch in range(epochs):
        indices = np.arange(n_samples)
        np.random.shuffle(indices)
        X_train = X_train[indices]
        y_train = y_train[indices]

        for t, (x, y) in enumerate(zip(X_train, y_train), start=1):
            x = x.reshape(1, -1)
            y = np.array([y])

            A3, cache = forward_pass(x, weights)

            gradients = backward_pass(y, weights, cache)

            lr = learning_rate_schedule(t, gamma_0, d)
            weights = update_weights(weights, gradients, lr)

        A3_train, _ = forward_pass(X_train, weights)
        train_loss = binary_cross_entropy(y_train, A3_train)

        A3_test, _ = forward_pass(X_test, weights)
        test_loss = binary_cross_entropy(y_test, A3_test)

        print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}")

    y_pred = (A3_test > 0.5).astype(int)
    accuracy = np.mean(y_pred == y_test.reshape(-1, 1))
    print(f"Hidden Size: {hidden_size}, Test Accuracy: {accuracy * 100:.2f}%")

if __name__ == "__main__":
    X_train, y_train, X_test, y_test = load_data("datasets/bank-note/train.csv", "/datasets/bank-note/test.csv")

    gamma_0 = 0.1
    d = 0.1
    epochs = 50

    for hidden_size in [5, 10, 25, 50, 100]:
        print(f"\nTraining with Hidden Size: {hidden_size}")
        train_neural_network(X_train, y_train, X_test, y_test, hidden_size, gamma_0, d, epochs)
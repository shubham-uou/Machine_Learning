# -*- coding: utf-8 -*-
"""SVM_DualDomain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15loPjvcIAHG0r_cDxvzZmYYItj0Sap90
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Load the training and testing datasets
train_df = pd.read_csv('/content/drive/My Drive/cs6350/assignments/assignment4/bank-note/train.csv', header=None)
test_df = pd.read_csv('/content/drive/My Drive/cs6350/assignments/assignment4/bank-note/test.csv', header=None)

# Extract features and labels from datasets
X_train = train_df.iloc[:, :-1].values
y_train = train_df.iloc[:, -1].values
X_test = test_df.iloc[:, :-1].values
y_test = test_df.iloc[:, -1].values

# Convert labels to {-1, 1}
y_train = np.where(y_train == 0, -1, 1)
y_test = np.where(y_test == 0, -1, 1)

# Hyperparameters
C_values = [100/873, 500/873, 700/873]  # Different values of C

# Dual SVM objective function
def dual_objective(alpha, X, y):
    # Matrix form of objective function
    y = y.reshape(-1, 1)
    K = np.dot(X, X.T) * (y @ y.T)
    return 0.5 * alpha @ K @ alpha - np.sum(alpha)

# Equality constraint: sum(alpha_i * y_i) = 0
def equality_constraint(alpha, y):
    return np.dot(alpha, y)

# Train SVM using dual form for different values of C
for C in C_values:
    n_samples = X_train.shape[0]

    # Initial guess for alpha
    alpha0 = np.zeros(n_samples)

    # Constraints and bounds
    constraints = ({'type': 'eq', 'fun': equality_constraint, 'args': (y_train,)})
    bounds = [(0, C) for _ in range(n_samples)]

    # Optimize using SLSQP
    result = minimize(dual_objective, alpha0, args=(X_train, y_train), method='SLSQP', bounds=bounds, constraints=constraints)
    alpha_opt = result.x

    # Recover the weights w
    w = np.sum((alpha_opt * y_train)[:, None] * X_train, axis=0)

    # Recover the bias b (using support vectors with 0 < alpha_i < C)
    support_vector_indices = (alpha_opt > 1e-5) & (alpha_opt < C - 1e-5)
    b = np.mean(y_train[support_vector_indices] - np.dot(X_train[support_vector_indices], w))

    # Report the weights and bias
    print(f"For C = {C}: Weights (w) = {w}, Bias (b) = {b}")

    # Calculate training and test errors
    train_predictions = np.sign(np.dot(X_train, w) + b)
    test_predictions = np.sign(np.dot(X_test, w) + b)
    train_error = np.mean(train_predictions != y_train)
    test_error = np.mean(test_predictions != y_test)

    # Report training and test errors
    print(f"For C = {C}: Training Error = {train_error:.4f}, Test Error = {test_error:.4f}")